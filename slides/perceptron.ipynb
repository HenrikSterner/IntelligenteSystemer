{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduktion til perceptroner\n",
    "## Af Henrik Sterner (hst@nextkbh.dk)\n",
    "\n",
    "Perceptroner er en type af neurale netværk, der er inspireret af biologiske neuroner. De er en af de ældste typer af neurale netværk, og blev opfundet i 1957 af Frank Rosenblatt. De udgør de basale byggesten og grundlaget for mange af de mere neurale netværk, der er blevet udviklet siden da.\n",
    "\n",
    "I det følgende vil vi se på, hvordan en perceptron fungerer, og hvordan den kan bruges til at løse et simple problemer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition af en perceptron\n",
    "\n",
    "En perceptron er en funktion, der tager en række inputværdier og beregner en outputværdi. Inputværdierne kan være binære, dvs. 0 eller 1, eller de kan være kontinuerte, dvs. reelle tal. Outputværdien er altid binær. \n",
    "\n",
    "Formulerer vi det matematisk, kan vi skrive det således:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2, \\ldots, x_n) = \\begin{cases}\n",
    "1 & \\text{hvis } \\sum_{i=1}^n w_i x_i + b > 0 \\\\\n",
    "0 & \\text{ellers}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Her er $x_1, x_2, \\ldots, x_n$ inputværdierne, $w_1, w_2, \\ldots, w_n$ er vægtene, og $b$ er en bias-værdi. \n",
    "\n",
    "Med andre ord: Vi beregner en vægtet sum af inputværdierne, lægger bias-værdien til, og hvis summen er større end 0, returnerer vi 1, ellers returnerer vi 0.\n",
    "\n",
    "\n",
    "Begrebet bias er indført for at vi på sigt kan flytte beslutningsgrænsen. Dvs. vi kan flytte beslutningsgrænsen op eller ned, så vi kan få en anden værdi end 0 til at være grænsen mellem 1 og 0. \n",
    "\n",
    "\n",
    "Vi kan også skrive det på vektorform:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\begin{cases}\n",
    "1 & \\text{hvis } \\mathbf{w} \\cdot \\mathbf{x} + b > 0 \\\\\n",
    "0 & \\text{ellers}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Her er $\\mathbf{x}$ en vektor med inputværdierne, $\\mathbf{w}$ er en vektor med vægtene, og $\\mathbf{w} \\cdot \\mathbf{x}$ er det indre produkt af $\\mathbf{w}$ og $\\mathbf{x}$. Dvs. \n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} = \\sum_{i=1}^n w_i x_i\n",
    "$$\n",
    "\n",
    "## Eksempel: AND-funktionen\n",
    "\n",
    "Lad os se på et eksempel. Vi vil gerne lave en perceptron, der kan fungere som en AND-gate. Dvs. den skal tage to inputværdier, $x_1$ og $x_2$, og returnere 1, hvis begge inputværdier er 1, og 0 ellers.\n",
    "\n",
    "Vi kan repræsentere AND-funktionen med en sandhedstabel:\n",
    "\n",
    "| $x_1$ | $x_2$ | $f(x_1, x_2)$ |\n",
    "|-------|-------|---------------|\n",
    "| 0     | 0     | 0             |\n",
    "| 0     | 1     | 0             |\n",
    "| 1     | 0     | 0             |\n",
    "| 1     | 1     | 1             |\n",
    "\n",
    "\n",
    "Vi kan nu finde en vægtvektor og en bias-værdi, der opfylder sandhedstabellen. Vi kan f.eks. vælge $\\mathbf{w} = [1, 1]$ og $b = -1.5$. Så får vi:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f([0, 0]) &= 1 \\cdot 0 + 1 \\cdot 0 - 1.5 = -1.5 < 0 \\\\\n",
    "f([0, 1]) &= 1 \\cdot 0 + 1 \\cdot 1 - 1.5 = -0.5 < 0 \\\\\n",
    "f([1, 0]) &= 1 \\cdot 1 + 1 \\cdot 0 - 1.5 = -0.5 < 0 \\\\\n",
    "f([1, 1]) &= 1 \\cdot 1 + 1 \\cdot 1 - 1.5 = 0.5 > 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Lad os se, om vi kan finde en anden vægtvektor og en anden bias-værdi, der også opfylder sandhedstabellen. Vi kan f.eks. vælge $\\mathbf{w} = [2, 2]$ og $b = -3$. Så får vi:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f([0, 0]) &= 2 \\cdot 0 + 2 \\cdot 0 - 3 = -3 < 0 \\\\\n",
    "f([0, 1]) &= 2 \\cdot 0 + 2 \\cdot 1 - 3 = -1 < 0 \\\\\n",
    "f([1, 0]) &= 2 \\cdot 1 + 2 \\cdot 0 - 3 = -1 < 0 \\\\\n",
    "f([1, 1]) &= 2 \\cdot 1 + 2 \\cdot 1 - 3 = 1 > 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Vi kan også vælge $\\mathbf{w} = [10, 10]$ og $b = -15$. Så får vi:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f([0, 0]) &= 10 \\cdot 0 + 10 \\cdot 0 - 15 = -15 < 0 \\\\\n",
    "f([0, 1]) &= 10 \\cdot 0 + 10 \\cdot 1 - 15 = -5 < 0 \\\\\n",
    "f([1, 0]) &= 10 \\cdot 1 + 10 \\cdot 0 - 15 = -5 < 0 \\\\\n",
    "f([1, 1]) &= 10 \\cdot 1 + 10 \\cdot 1 - 15 = 5 > 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Læg mærke til, at alle tre vægtvektorer og bias-værdier opfylder sandhedstabellen. Det er derfor ikke muligt at sige, hvilken vægtvektor og bias-værdi, der er den rigtige. Der er mange mulige løsninger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Læring af vægtene\n",
    "\n",
    "Læring af vægtene i en perceptron foregår ved at præsentere den for en række eksempler, og så justere vægtene, så de passer til eksemplerne.\n",
    "\n",
    "Vi kan f.eks. præsentere vores AND-perceptron for eksemplerne $[0, 0]$, $[0, 1]$, $[1, 0]$ og $[1, 1]$, og så justere vægtene, så de passer til eksemplerne.\n",
    "\n",
    "Vi indfører begrebet læringsrate, som vi kalder $\\eta$. Læringsraten er et tal mellem 0 og 1, og den bestemmer, hvor meget vi justerer vægtene. Hvis vi f.eks. har en læringsrate på 0.1, så justerer vi vægtene med 10% af forskellen mellem den ønskede outputværdi og den faktiske outputværdi.\n",
    "\n",
    "Vi kan f.eks. præsentere vores AND-perceptron for eksemplerne $[0, 0]$, $[0, 1]$, $[1, 0]$ og $[1, 1]$, og så justere vægtene, så de passer til eksemplerne.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoker (epocs)\n",
    "\n",
    "En epoke er en gennemløbning af alle eksemplerne i træningssættet. Hvis vi har 100 eksempler i træningssættet, og vi præsenterer dem for perceptronen én gang, så har vi gennemført én epoke. Hvis vi præsenterer dem for perceptronen 10 gange, så har vi gennemført 10 epoker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En perceptron i Python\n",
    "\n",
    "Lad os nu implementere en perceptron i Python. Vi starter med at importere NumPy-biblioteket, som vi skal bruge til at arbejde med vektorer og matricer, og dernæst konstruerer vi en klasse, der repræsenterer en perceptron med en række forskellige metoder og attributter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron.py\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, input_size, lr=1, epochs=100):\n",
    "        self.W = np.zeros(input_size+1)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "    \n",
    "    def activation_fn(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)\n",
    "        z = self.W.T.dot(x)\n",
    "        a = self.activation_fn(z)\n",
    "        return a\n",
    "    \n",
    "    def fit(self, X, d):\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(d.shape[0]):\n",
    "                y = self.predict(X[i])\n",
    "                e = d[i] - y\n",
    "                self.W = self.W + self.lr * e * np.insert(X[i], 0, 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
